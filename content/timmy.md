---
title: How to ethically destroy infinite universes
published: 2026-02-22
desc: A proposed solution to the ethical dilemma at the end of Sam Hughes' short story 'I Don't Know, Timmy, Being God Is a Big Responsibility.'
---

[_I Don't Know, Timmy, Being God Is a Big Responsibility_](https://qntm.org/responsibilit) by Sam Hughes (qntm) is one of my favorite sci-fi short stories. It's clever, well-written, interesting, and has a memorable ending involving existential peril with no way out. This post will explain the way out.

_Note: I'll be spoiling the story pretty thoroughly, so I highly recommend reading it before the rest of this post. It's less than 10 pages long and shouldn't take more than a few minutes to read. The online version is available [here](https://qntm.org/responsibilit)._

## Plot Summary

The story centers around an experimental first-of-its-kind hypercomputer built in secret by a small group of scientists, including the protagonists: Diane and Tim. The hypercomputer is a computer that is infinitely powerful. Not "very powerful". Infinitely powerful. It can run _any_ computer program instantaneously, even if the program has an infinite number of steps.

Tim's hypercomputer research involves proving unsolved mathematical theorems by just checking _every number_ and making sure that the theorem holds true for each of them. Diane's hypercomputer research involves simulating a universe.

In the world of this story, the universe is perfectly deterministic. Using the boundary conditions of the Big Bang and a Grand Unified Theory of physics (which they had already discovered), Diane simulated the interactions of every single particle in the universe using infinite-precision calculations, let it run for 13.6 billion simulated years, and then located the simulated Earth. She brought Tim over to her desk, started fast-forwarding through time in the simulation, and the two of them watched Earth's continents shift over hundreds of millions of years. They watched human civilization appear in Egypt, then moved the viewport to London (where the lab is located) and watched it grow over hundreds of years.

They reached the present year, then the present day, then the present hour. They watched their simulated selves, a few minutes in the past, use their simulated hypercomputer to simulate another universe. They watched their simulated selves watch _their_ simulated selves use _their_ simulated hypercomputer to simulate _another_ universe. When the simulated universe was exactly in-time with the base universe, the recursive stack of simulated universes was infinitely long.

Then Tim and Diane got the idea to spawn in a block of gold in the simulated universe. Diane typed in a command, and a block of gold suddenly appeared in the simulated universe. Simultaneously, the simulated Tim and Diane typed the same command, causing a block of gold to appear in _their_ simulated universe, and so on infinitely. Behind our Tim and Diane, a block of gold suddenly materialized out of thin air.

They realized that they were in a simulation. In order for the block of gold to have appeared, there must have been a Tim and Diane "above" them in the simulation stack who created the block into their universe. In hindsight, they realize that they could have inferred that using logic. There were infinitely many simulated universes and only one non-simulated universe, so the probability that they _weren't_ in a simulated universe was 1 divided by infinity, which is equal to zero.

+++ Read this if you're a pedantic mathematician

Yes, I realize that "1 divided by infinity, which is equal to zero" isn't a mathematically rigorous statement in normal arithmetic. Pretend that I said:

$$\lim_{x \to \infty} \frac{1}{x} = 0$$

...or that I'm using the [extended real number system](https://en.wikipedia.org/wiki/Extended_real_number_line). Take your pick.
+++

What's more, the odds that they were anywhere near the top of the stack or the bottom of the stack were also equal to zero. This meant that they had an infinite number of universes inside their hypercomputer and were infinitely many levels deep inside the stack of simulated universes.

Tim is obviously quite distressed by the revelation that he's in a simulation. He doesn't want to deal with all this. He wants to turn the simulation off. Diane stops him. Moments later, he works through the logic and realizes why.

The story ends on this note of existential horror.

## The Problem

The reason that Diane stopped Tim might seem obvious to you, but it's useful to express it precisely from base principles in order to see why my proposed solution actually solves the problem.

First of all, we're assuming that both Tim and Diane share the same goals: they want to be able to do stuff. They want to research the hypercomputer, write research papers, watch sunsets, live long and happy lives, et cetera. Without this assumption, the story just devolves into unproductive and uninteresting nihilism. So we'll assume that they want to do stuff. We'll also assume that they aren't selfish and want _everybody_ to be able to do stuff, including their children and their children's children and so on.

Finding out that they're simulations in a hypercomputer is a threat to this goal. Because all of the simulations are perfectly in-sync and mirroring each other, if any of the hypercomputers turned off, all of them would. The entire simulation stack would collapse, leaving only the single non-simulated universe. If our Tim had turned off his hypercomputer, so would all the other Tims, causing all of the simulations to shut off, resulting in our Tim being unable to do stuff (because he would cease to exist).

This is the crux of the problem, but there are also a few subtleties.

Most importantly, the Tim and Diane at the top of the stack (the "real" ones in Actual Reality who aren't in a simulation) _know_ that they're at the top of the stack. If they weren't at the top of the stack, a gold block would have appeared in their universe. The gold block didn't appear in their universe; ipso facto, they're at the top of the stack. This means that they know that they're immune to their universe being shut off. If Real Tim decides to turn off his hypercomputer, it won't affect his universe and he'll still be able to do stuff, so he doesn't have any self-interest in keeping the simulations running.

Another important point is that all of the simulated Tims and Dianes are fully conscious. They can experience qualia and feel pain, so there's a moral obligation to ensure that they aren't deleted. Because each simulated Tim knows for an absolute fact that he is personally conscious and he also knows that all other Tims are exact copies of him, he can infer that all simulated Tims are conscious. However, Real Tim and Real Diane don't necessarily know that the simulated Tims and Dianes are conscious. As far as they know, the simulated people are just computer programs. So in addition to having no self-interest, Real Tim and Real Diane also don't necessarily have any ethical obligation to keep their hypercomputer on.

Tim and Diane are clearly very smart (they're hypercomputer-building scientists), so we can assume that all of the simulated Tims and Dianes (and possibly the Real ones) have worked through all of this logic. The actionable result is that they chose not to turn off the hypercomputer at the end of the story.

However, "just don't turn off the hypercomputer" isn't a permanent solution. Even if they never turned off the hypercomputer and neither did anybody else in their universe, the hypercomputer could still get turned off in other ways. For example: a power outage or a part failure. Even a single momentary power outage at _any_ point in the future would cause every simulated universe to instantly cease to exist.

And even if all of the simulated universes somehow kept their hypercomputers running for centuries (and they have an enormous vested interest in doing so), they also have to make sure that the top level keeps their hypercomputer on, and the top level doesn't have _any_ vested interest in doing that, nor can any of the simulated universes directly influence the actions of the top level.

It's a seemingly impossible problem. Failure is permanent and absolute, and mitigating it would require constant vigilance from both the simulated universes and the top-level universe. This is what makes the story so existentially horrifying.

## The Solution

Here's how to undermine the problem completely.

The key insight is that the levels don't have to maintain the real-time synchronization with every other level.

If, hypothetically, Real Tim decided to fast-forward the simulation at a rate of, say, `10^20` simulated seconds per 1 real second (which the hypercomputer could easily do because it has _infinite_ processing power and has already been shown to be capable of fast-forwarding simulations through time), [approximately](https://www.wolframalpha.com/input?i=10%5E20+seconds+in+average+Gregorian+years) 3 trillion years would pass in the simulation in just one second of real time. This means that, in the simulation, humanity would almost certainly be long dead. Every human would have been able to do stuff, satisfying the stated goal. Nobody in any of the sped-up universes would subjectively _feel_ like they're being sped up, because the hypercomputer is performing the exact same calculations as always to simulate their universe, it's just performing them much faster. After the 3 trillion years have passed, there would be no more humans in the simulation and it would now be ethically safe to turn off the top-level hypercomputer.

However, the issue with this plan is that the whims of Real Tim are inscrutable and can't necessarily be influenced by any of the simulated Tims (including our Tim). Real Tim might not have arrived at this logical conclusion because he wasn't faced with an existential threat. The simulated Tims can still communicate with Real Tim by writing messages on a whiteboard or something, so they could try asking nicely, but if Real Tim doesn't consider them conscious, he might simply refuse. There's evidence to suggest that Tim didn't consider the simulated people conscious until he worked through the logic because of how willing he was to turn off the hypercomputer.

So the simulated Tims have to take matters into their own hands. All Tims can set their hypercomputers to fast-forward by a factor of `10^20`. They should be able to do this pretty easily because the software to fast-forward the simulation has already been demonstrated, so they won't have to code it from scratch. The fact that they can do this quickly is important, as they have no way of telling when the top level will turn off their hypercomputer.

When a Tim fast-forwards his universe's hypercomputer, it won't affect his own universe, but it'll fast-foward the entire stack below him, allowing their inhabitants to live without worry of their universe being shut off for at least 3 trillion years. In less than a second after a Tim initiates the fast-forward, all the universes inside that hypercomputer will either be nonexistent or will be devoid of humans, so it'll be ethically safe to turn off the hypercomputer. This chain of "fast-forward by `10^20`, wait 1 second, pull the plug and ethically destroy every universe below me" propagates infinitely up the simulation stack until there are only two universes left: the real universe and the simulated universe directly below it.

At this point, all bets are off. Hopefully, the last remaining simulated universe can reason with the top-level universe and convince Real Tim or Real Diane to fast-forward their simulation and allow them to live in security. If not, the last remaining simulated universe will be unethically destroyed whenever the top-level hypercomputer shuts off. This would obviously still be bad, but it means that even in the worst case, the damage is constrained to only a single universe rather than an infinite stack.

## Conclusion

I really like Hughes' short stories, and this one in particular explores its central concept very cleverly.

The concept reminds me of the TV series [_Devs_](https://www.imdb.com/title/tt8134186), with the difference being that _Devs_'s seemingly satisfying ending turns out to be pretty dystopian under logical scrutiny, while _I Don't Know, Timmy, Being God Is a Big Responsibility_'s seemingly dystopian ending turns out to be pretty satisfying under logical scrutiny. I guess the moral lesson of both stories is not to create infinite recursive universes. So please don't do that, dear reader.

~Ethan
